{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e7b009",
   "metadata": {},
   "source": [
    "# üìò RAG Pipeline Notebook (LangChain + LangGraph + Groq + Open-Source Embeddings)\n",
    "\n",
    "This notebook contains the full pipeline structure. Fill in sections as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c96622",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35a9b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: langgraph in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: ragas in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.6)\n",
      "Requirement already satisfied: pypdf in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: pinecone-text in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.1.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (0.2.12)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.53)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (23.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (8.4.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.12.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: groq<1.0.0,>=0.30.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-groq) (0.37.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (4.4.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (0.7.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (1.1.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (1.109.1)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-text) (4.1.0)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.9.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-text) (3.9.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.25.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-text) (2.32.0.20240712)\n",
      "Requirement already satisfied: joblib in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk<4.0.0,>=3.9.1->pinecone-text) (1.4.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>1->ragas) (0.4.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets->ragas) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets->ragas) (2.2.2)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\junke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-groq sentence-transformers faiss-cpu langgraph ragas pypdf spacy rank-bm25 transformers accelerate pinecone-client pinecone-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42d0a85b-f1f6-4b48-8533-db84cfe4cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------- ------------------ 6.8/12.8 MB 42.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 42.2 MB/s  0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a0361",
   "metadata": {},
   "source": [
    "## 2. Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56eb96ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "pdf_path = \"Deepseek-r1.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76925ef",
   "metadata": {},
   "source": [
    "## 3. Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a1ebf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"-\\n\", \"\", text)  # fix hyphen-newlines\n",
    "    text = re.sub(r\"\\n\", \" \", text)  # flatten newlines\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_docs = []\n",
    "for d in docs:\n",
    "    cleaned_docs.append(clean_text(d.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcb60f",
   "metadata": {},
   "source": [
    "## 4. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb59696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed chunks: 136\n",
      "Semantic chunks: 75\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import spacy\n",
    "\n",
    "# load spacy for semantic chunking\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Fixed Chunking\n",
    "# ---------------------------\n",
    "fixed_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "fixed_chunks = fixed_splitter.create_documents(cleaned_docs)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Semantic Chunking\n",
    "# ---------------------------\n",
    "def semantic_chunk(text, max_tokens=120):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        current.append(sent.text)\n",
    "        if len(\" \".join(current).split()) > max_tokens:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = []\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "semantic_chunks = []\n",
    "for d in cleaned_docs:\n",
    "    semantic_chunks.extend(semantic_chunk(d))\n",
    "\n",
    "print(\"Fixed chunks:\", len(fixed_chunks))\n",
    "print(\"Semantic chunks:\", len(semantic_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99bca0",
   "metadata": {},
   "source": [
    "## 5.1. Initialize Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f142ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Embedding model loaded, dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877bb9",
   "metadata": {},
   "source": [
    "## 5.2. Pinecone Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "413ea6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Get Pinecone API key from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"Please set PINECONE_API_KEY environment variable or configure it in .env file\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"rag-semantic-index\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embed_model.get_sentence_embedding_dimension(),\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72f05b21-9495-48c6-816e-3307b951e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded 75/75 vectors\n",
      "‚úÖ All vectors uploaded, total: 75\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "batch_size = 100  # Process in batches to avoid uploading too many at once\n",
    "\n",
    "for i, text in enumerate(semantic_chunks):\n",
    "    emb = embed_model.encode(text, show_progress_bar=False).tolist()\n",
    "    vectors.append({\n",
    "        \"id\": str(i),\n",
    "        \"values\": emb,\n",
    "        \"metadata\": {\"text\": text[:500]}  # Limit metadata length\n",
    "    })\n",
    "\n",
    "# Batch upload\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    batch = vectors[i:i + batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "    print(f\"Uploaded {min(i + batch_size, len(vectors))}/{len(vectors)} vectors\")\n",
    "\n",
    "print(f\"All vectors uploaded, total: {len(vectors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa1c1",
   "metadata": {},
   "source": [
    "## 5.3. Reranker Ê®°Âûã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4013dae9-cd4a-4f1b-9533-ff591ec8146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BM25\n",
    "tokenized_corpus = [doc.split() for doc in semantic_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def vector_search_pinecone(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Perform vector search using Pinecone.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of matching results from Pinecone\n",
    "    \"\"\"\n",
    "    q_emb = embed_model.encode(query, show_progress_bar=False).tolist()\n",
    "    res = index.query(\n",
    "        vector=q_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return res.get(\"matches\", [])\n",
    "\n",
    "def hybrid_search(query, alpha=0.5, top_k=10):\n",
    "    \"\"\"\n",
    "    Hybrid search combining BM25 and vector search.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        alpha: BM25 weight (0-1), alpha=1 means only BM25, alpha=0 means only vector search\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: [(index, hybrid_score, text), ...]\n",
    "    \"\"\"\n",
    "    # BM25 scores\n",
    "    query_tokens = query.split()\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Normalize BM25 scores\n",
    "    if np.max(bm25_scores) - np.min(bm25_scores) > 1e-9:\n",
    "        bm25_norm = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
    "    else:\n",
    "        bm25_norm = np.ones_like(bm25_scores) * 0.5\n",
    "\n",
    "    # Pinecone vector search\n",
    "    vector_results = vector_search_pinecone(query, top_k=top_k * 2)  # Get more candidates\n",
    "    \n",
    "    vector_scores = np.zeros(len(semantic_chunks))\n",
    "    for m in vector_results:\n",
    "        idx = int(m[\"id\"])\n",
    "        if 0 <= idx < len(semantic_chunks):\n",
    "            vector_scores[idx] = m.get(\"score\", 0.0)\n",
    "\n",
    "    # Normalize vector scores\n",
    "    if np.max(vector_scores) - np.min(vector_scores) > 1e-9:\n",
    "        vector_norm = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores))\n",
    "    else:\n",
    "        vector_norm = np.ones_like(vector_scores) * 0.5\n",
    "\n",
    "    # Hybrid score\n",
    "    hybrid = alpha * bm25_norm + (1 - alpha) * vector_norm\n",
    "\n",
    "    # Top K results\n",
    "    best_idx = np.argsort(hybrid)[::-1][:top_k]\n",
    "\n",
    "    return [(i, float(hybrid[i]), semantic_chunks[i]) for i in best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e5481a6-effe-4390-9db0-9ff588477698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reranker model...\n",
      "Reranker model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading Reranker model...\")\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"BAAI/bge-reranker-base\"\n",
    ")\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\")\n",
    "reranker_model.eval()  # Set to evaluation mode\n",
    "print(\"Reranker model loaded\")\n",
    "\n",
    "def rerank(query, candidates, top_k=5):\n",
    "    \"\"\"\n",
    "    Rerank candidate results using Reranker model.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        candidates: list of (index, hybrid_score, text)\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: [(rerank_score, (index, hybrid_score, text)), ...]\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    pairs = [[query, c[2]] for c in candidates]\n",
    "    inputs = reranker_tokenizer(\n",
    "        pairs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,  # Limit max length\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze()\n",
    "    \n",
    "    # Handle single result case\n",
    "    if scores.dim() == 0:\n",
    "        scores = scores.unsqueeze(0)\n",
    "    \n",
    "    scored = list(zip(scores.tolist(), candidates))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return scored[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d12f4",
   "metadata": {},
   "source": [
    "## 6. Groq LLM (RAG Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3bd651c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq LLM initialized\n",
      "\n",
      "Test query: What is the main contribution of DeepSeek-R1?\n",
      "\n",
      "Answer:\n",
      "The context does not contain a direct statement about the main contribution of DeepSeek-R1. However, based on the information provided, it can be inferred that DeepSeek-R1 is an improvement over DeepSeek-R1-Zero, with a focus on making the reasoning processes more readable and aligning with human preferences. The main contribution of DeepSeek-R1 seems to be its ability to produce clear and coherent Chains of Thought (CoT) while demonstrating strong general capabilities, achieved through a pipeline that incorporates reinforcement learning with human-friendly cold-start data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Groq API key from environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# If not found, prompt user to set it\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"GROQ_API_KEY not found in environment variables.\")\n",
    "    print(\"Please do one of the following:\")\n",
    "    print(\"1. Create a .env file in the project root with: GROQ_API_KEY=your_key_here\")\n",
    "    print(\"2. Or set it directly in this cell (temporary, for development only):\")\n",
    "    print(\"   GROQ_API_KEY = 'your_groq_api_key_here'\")\n",
    "    print(\"\\nTo get your Groq API key, visit: https://console.groq.com/\")\n",
    "    raise ValueError(\"Please set GROQ_API_KEY environment variable or configure it in .env file\")\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.1,  # Lower temperature for more stable output\n",
    "    max_tokens=1000\n",
    ")\n",
    "print(\"Groq LLM initialized\")\n",
    "\n",
    "def retrieve(query, top_k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        top_k: Number of top documents to return\n",
    "        alpha: BM25 weight for hybrid search\n",
    "    \n",
    "    Returns:\n",
    "        list: List of retrieved document texts\n",
    "    \"\"\"\n",
    "    # Hybrid search\n",
    "    hybrid_results = hybrid_search(query, alpha=alpha, top_k=20)\n",
    "    \n",
    "    if not hybrid_results:\n",
    "        print(\"No relevant documents found\")\n",
    "        return []\n",
    "    \n",
    "    # Rerank\n",
    "    reranked_results = rerank(query, hybrid_results, top_k=top_k)\n",
    "    \n",
    "    # Extract texts\n",
    "    retrieved_texts = [r[1][2] for r in reranked_results]\n",
    "    return retrieved_texts\n",
    "    \n",
    "def rag_answer(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        top_k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    ctx = retrieve(query, top_k=top_k)\n",
    "    \n",
    "    if not ctx:\n",
    "        return \"Sorry, no relevant information found.\"\n",
    "    \n",
    "    # Build prompt\n",
    "    context_text = \"\\n\\n\".join([f\"[Document {i+1}]: {text}\" for i, text in enumerate(ctx)])\n",
    "    \n",
    "    prompt = f\"\"\"Answer the question based on the following context. If the context does not contain relevant information, please state so.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Extract text content\n",
    "    if hasattr(response, 'content'):\n",
    "        return response.content\n",
    "    else:\n",
    "        return str(response)\n",
    "\n",
    "# Test\n",
    "test_query = \"What is the main contribution of DeepSeek-R1?\"\n",
    "print(f\"\\nTest query: {test_query}\")\n",
    "answer = rag_answer(test_query)\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c729f30",
   "metadata": {},
   "source": [
    "## 6.1. Multi-turn Conversation Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0e519845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üí¨ Multi-turn Conversation Example\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ConversationManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create a new conversation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m conv = \u001b[43mConversationManager\u001b[49m(top_k=\u001b[32m5\u001b[39m, alpha=\u001b[32m0.5\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Turn 1: Initial question\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Turn 1]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ConversationManager' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: Multi-turn conversation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí¨ Multi-turn Conversation Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a new conversation\n",
    "conv = ConversationManager(top_k=5, alpha=0.5)\n",
    "\n",
    "# Turn 1: Initial question\n",
    "print(\"\\n[Turn 1]\")\n",
    "q1 = \"What is DeepSeek-R1?\"\n",
    "print(f\"User: {q1}\")\n",
    "a1 = conv.chat(q1)\n",
    "print(f\"Assistant: {a1}\")\n",
    "\n",
    "# Turn 2: Follow-up question\n",
    "print(\"\\n[Turn 2]\")\n",
    "q2 = \"How does it compare to other models?\"\n",
    "print(f\"User: {q2}\")\n",
    "a2 = conv.chat(q2)\n",
    "print(f\"Assistant: {a2}\")\n",
    "\n",
    "# Turn 3: Another follow-up\n",
    "print(\"\\n[Turn 3]\")\n",
    "q3 = \"What are its main features?\"\n",
    "print(f\"User: {q3}\")\n",
    "a3 = conv.chat(q3)\n",
    "print(f\"Assistant: {a3}\")\n",
    "\n",
    "# Display full conversation history\n",
    "conv.display_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed24a06",
   "metadata": {},
   "source": [
    "## 6.2. Interactive Conversation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43469ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat loop for multi-turn conversation.\n",
    "    Type 'quit', 'exit', or 'reset' to end or reset conversation.\n",
    "    \"\"\"\n",
    "    conv = ConversationManager(top_k=5, alpha=0.5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí¨ Interactive RAG Chat\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Type your questions below. Commands:\")\n",
    "    print(\"  - 'quit' or 'exit': End conversation\")\n",
    "    print(\"  - 'reset': Clear conversation history\")\n",
    "    print(\"  - 'history': Show conversation history\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Handle commands\n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'reset':\n",
    "                conv.reset()\n",
    "                print(\"‚úÖ Conversation history cleared\\n\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'history':\n",
    "                conv.display_history()\n",
    "                continue\n",
    "            \n",
    "            # Process query\n",
    "            print(\"ü§î Thinking...\")\n",
    "            answer = conv.chat(user_input)\n",
    "            print(f\"\\nAssistant: {answer}\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Uncomment the line below to start interactive chat\n",
    "# interactive_chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee10781",
   "metadata": {},
   "source": [
    "## 6.3. Programmatic Conversation API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic conversation example\n",
    "# Useful for integrating RAG into applications\n",
    "\n",
    "def create_conversation_session():\n",
    "    \"\"\"\n",
    "    Create a new conversation session.\n",
    "    \n",
    "    Returns:\n",
    "        ConversationManager: New conversation manager instance\n",
    "    \"\"\"\n",
    "    return ConversationManager(top_k=5, alpha=0.5)\n",
    "\n",
    "# Example usage in an application\n",
    "def example_application():\n",
    "    \"\"\"\n",
    "    Example of how to use ConversationManager in an application.\n",
    "    \"\"\"\n",
    "    # Create session\n",
    "    session = create_conversation_session()\n",
    "    \n",
    "    # Simulate user interactions\n",
    "    queries = [\n",
    "        \"What is DeepSeek-R1?\",\n",
    "        \"What are its key capabilities?\",\n",
    "        \"How does it perform on reasoning tasks?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì± Application Example\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n[Request {i}]\")\n",
    "        print(f\"User Query: {query}\")\n",
    "        \n",
    "        # Get answer\n",
    "        answer = session.chat(query)\n",
    "        \n",
    "        print(f\"Response: {answer[:200]}...\")  # Truncate for display\n",
    "    \n",
    "    # Get conversation summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä Conversation Summary\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total turns: {len(session.get_history())}\")\n",
    "    session.display_history()\n",
    "\n",
    "# Run example\n",
    "example_application()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5e966-b547-4948-b558-0a724428d212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9, 'relevance': 0.9, 'context_recall': 0.7536224}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --------------------------------\n",
    "# 1. Groq Judge LLM Wrapper\n",
    "# --------------------------------\n",
    "class GroqJudge:\n",
    "    \"\"\"\n",
    "    Groq LLM wrapper for evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model=\"llama-3.3-70b-versatile\"):\n",
    "        self.llm = ChatGroq(model=model, temperature=0.0)  # Deterministic output for evaluation\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        response = self.llm.invoke(prompt)\n",
    "        if hasattr(response, 'content'):\n",
    "            return response.content\n",
    "        return str(response)\n",
    "\n",
    "\n",
    "judge_llm = GroqJudge()\n",
    "print(\"‚úÖ Groq Judge initialized\")\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# 2. Faithfulness Metric (LLM-Judge)\n",
    "# --------------------------------\n",
    "def evaluate_faithfulness(answer, contexts):\n",
    "    \"\"\"\n",
    "    Evaluate how faithful the answer is to the context.\n",
    "    \n",
    "    Args:\n",
    "        answer: Generated answer (string)\n",
    "        contexts: List of context texts\n",
    "    \n",
    "    Returns:\n",
    "        float: Faithfulness score (0-1)\n",
    "    \"\"\"\n",
    "    # Ensure answer is a string\n",
    "    if hasattr(answer, 'content'):\n",
    "        answer = answer.content\n",
    "    answer = str(answer)\n",
    "    \n",
    "    # Format contexts\n",
    "    contexts_text = \"\\n\\n\".join([f\"[Document {i+1}]: {ctx}\" for i, ctx in enumerate(contexts)])\n",
    "    \n",
    "    prompt = f\"\"\"You are an evaluator. Evaluate how faithful the answer is to the context (whether the answer is based on the context without fabricating information).\n",
    "\n",
    "Context:\n",
    "{contexts_text}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Give a score between 0 and 1, where:\n",
    "- 1.0: Answer is completely based on context, no fabricated information\n",
    "- 0.5: Answer is partially based on context, but contains some fabrication or errors\n",
    "- 0.0: Answer is unrelated to context or contains significant errors\n",
    "\n",
    "Output only the number (a decimal between 0 and 1).\"\"\"\n",
    "    \n",
    "    score_text = judge_llm(prompt)\n",
    "    # Extract number\n",
    "    score_match = re.search(r'0?\\.\\d+|1\\.0|0', score_text)\n",
    "    if score_match:\n",
    "        score = float(score_match.group())\n",
    "        return max(0.0, min(1.0, score))  # Clamp to 0-1\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# 3. Relevance Metric (LLM-Judge)\n",
    "# --------------------------------\n",
    "def evaluate_relevance(question, answer):\n",
    "    \"\"\"\n",
    "    Evaluate how well the answer responds to the question.\n",
    "    \n",
    "    Args:\n",
    "        question: Question text\n",
    "        answer: Answer text\n",
    "    \n",
    "    Returns:\n",
    "        float: Relevance score (0-1)\n",
    "    \"\"\"\n",
    "    # Ensure answer is a string\n",
    "    if hasattr(answer, 'content'):\n",
    "        answer = answer.content\n",
    "    answer = str(answer)\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate how well the answer responds to the question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Give a score between 0 and 1, where:\n",
    "- 1.0: Answer completely addresses the question, accurate and complete\n",
    "- 0.5: Answer partially addresses the question, but incomplete or inaccurate\n",
    "- 0.0: Answer does not address the question or is completely irrelevant\n",
    "\n",
    "Output only the number (a decimal between 0 and 1).\"\"\"\n",
    "    \n",
    "    score_text = judge_llm(prompt)\n",
    "    # Extract number\n",
    "    score_match = re.search(r'0?\\.\\d+|1\\.0|0', score_text)\n",
    "    if score_match:\n",
    "        score = float(score_match.group())\n",
    "        return max(0.0, min(1.0, score))\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# 4. Context Recall (Embedding Similarity)\n",
    "# --------------------------------\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        a: First vector\n",
    "        b: Second vector\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine similarity score\n",
    "    \"\"\"\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    return np.dot(a, b) / (norm_a * norm_b)\n",
    "\n",
    "def evaluate_context_recall(question, contexts, embed_model):\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of retrieved contexts to the question (recall metric).\n",
    "    \n",
    "    Args:\n",
    "        question: Question text\n",
    "        contexts: List of context texts\n",
    "        embed_model: Embedding model\n",
    "    \n",
    "    Returns:\n",
    "        float: Maximum similarity score (0-1)\n",
    "    \"\"\"\n",
    "    if not contexts:\n",
    "        return 0.0\n",
    "    \n",
    "    q_emb = embed_model.encode(question, show_progress_bar=False)\n",
    "    ctx_embs = embed_model.encode(contexts, show_progress_bar=False)\n",
    "    \n",
    "    sims = [cosine_similarity(q_emb, c) for c in ctx_embs]\n",
    "    return float(max(sims)) if sims else 0.0\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# 5. Combine all metrics into one evaluator\n",
    "# --------------------------------\n",
    "def evaluate_rag(query, answer, contexts, embed_model):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of RAG system performance.\n",
    "    \n",
    "    Args:\n",
    "        query: Query text\n",
    "        answer: Generated answer\n",
    "        contexts: List of retrieved context texts\n",
    "        embed_model: Embedding model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Starting RAG system evaluation...\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Retrieved {len(contexts)} contexts\")\n",
    "    \n",
    "    results = {\n",
    "        \"faithfulness\": evaluate_faithfulness(answer, contexts),\n",
    "        \"relevance\": evaluate_relevance(query, answer),\n",
    "        \"context_recall\": evaluate_context_recall(query, contexts, embed_model)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed:\")\n",
    "    print(f\"  - Faithfulness: {results['faithfulness']:.3f}\")\n",
    "    print(f\"  - Relevance: {results['relevance']:.3f}\")\n",
    "    print(f\"  - Context Recall: {results['context_recall']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ Testing evaluation functionality\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"What is DeepSeek-R1?\"\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "\n",
    "# Generate answer\n",
    "answer = rag_answer(test_query)\n",
    "print(f\"\\nGenerated answer:\\n{answer}\")\n",
    "\n",
    "# Retrieve contexts\n",
    "contexts = retrieve(test_query)\n",
    "print(f\"\\nNumber of retrieved contexts: {len(contexts)}\")\n",
    "\n",
    "# Evaluate\n",
    "evaluation_results = evaluate_rag(test_query, answer, contexts, embed_model)\n",
    "print(f\"\\nEvaluation results: {evaluation_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988b1d9-3930-41fe-8985-13217a71244d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
