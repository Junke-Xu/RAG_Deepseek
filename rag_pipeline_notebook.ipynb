{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Pipeline (LangChain + LangGraph + Groq + Open-Source Embeddings)\n\n",
        "This notebook provides a structured, end-to-end RAG pipeline with clear sections and reusable functions.\n",
        "Everything is written in English and avoids emojis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment and Dependencies\n",
        "Install required packages if you are running this notebook in a fresh environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# You may comment this cell if dependencies are already installed\n",
        "!pip install -q langchain langchain-community langchain-groq sentence-transformers faiss-cpu langgraph ragas pypdf spacy rank-bm25 transformers accelerate pinecone-client pinecone-text python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Download the spaCy English model (run once)\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "Load environment variables and validate API keys (Groq and Pinecone)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "if not GROQ_API_KEY:\n",
        "    raise ValueError(\"Please set GROQ_API_KEY in your environment or .env file\")\n",
        "\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "if not PINECONE_API_KEY:\n",
        "    raise ValueError(\"Please set PINECONE_API_KEY in your environment or .env file\")\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "print(\"Configuration loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Document Loading and Cleaning\n",
        "Load a PDF and normalize its text content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "pdf_path = \"Deepseek-r1.pdf\"  # Place your PDF in the project root or adjust the path\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} pages\")\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r\"-\\n\", \"\", text)  # fix hyphen-newlines\n",
        "    text = re.sub(r\"\\n\", \" \", text)  # flatten newlines\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "cleaned_docs = [clean_text(d.page_content) for d in docs]\n",
        "print(\"Text cleaning complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chunking\n",
        "Use both fixed-size chunking and semantic chunking (with spaCy sentences)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Fixed chunking\n",
        "fixed_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "fixed_chunks = fixed_splitter.create_documents(cleaned_docs)\n",
        "\n",
        "# Semantic chunking by sentences with token cap\n",
        "def semantic_chunk(text: str, max_tokens: int = 120):\n",
        "    doc = nlp(text)\n",
        "    chunks, current = [], []\n",
        "    for sent in doc.sents:\n",
        "        current.append(sent.text)\n",
        "        if len(\" \".join(current).split()) > max_tokens:\n",
        "            chunks.append(\" \".join(current))\n",
        "            current = []\n",
        "    if current:\n",
        "        chunks.append(\" \".join(current))\n",
        "    return chunks\n",
        "\n",
        "semantic_chunks = []\n",
        "for d in cleaned_docs:\n",
        "    semantic_chunks.extend(semantic_chunk(d))\n",
        "\n",
        "print(f\"Fixed chunks: {len(fixed_chunks)}\")\n",
        "print(f\"Semantic chunks: {len(semantic_chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embeddings, Vector Store (Pinecone), BM25, and Reranker\n",
        "Initialize embedding model, Pinecone index, upload vectors, BM25 corpus, and the reranker model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading embedding model...\")\n",
        "embed_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
        "print(f\"Embedding model dimension: {embed_model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "# Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = \"rag-semantic-index\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embed_model.get_sentence_embedding_dimension(),\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Upsert vectors\n",
        "vectors, batch_size = [], 100\n",
        "for i, text in enumerate(semantic_chunks):\n",
        "    emb = embed_model.encode(text, show_progress_bar=False).tolist()\n",
        "    vectors.append({\"id\": str(i), \"values\": emb, \"metadata\": {\"text\": text[:500]}})\n",
        "\n",
        "for i in range(0, len(vectors), batch_size):\n",
        "    batch = vectors[i:i+batch_size]\n",
        "    index.upsert(vectors=batch)\n",
        "    print(f\"Uploaded {min(i+batch_size, len(vectors))}/{len(vectors)} vectors\")\n",
        "print(f\"All vectors uploaded, total: {len(vectors)}\")\n",
        "\n",
        "# BM25\n",
        "tokenized_corpus = [doc.split() for doc in semantic_chunks]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready\")\n",
        "\n",
        "# Reranker\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"Loading reranker...\")\n",
        "reranker_model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-base\")\n",
        "reranker_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\")\n",
        "reranker_model.eval()\n",
        "print(\"Reranker ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Retrieval Utilities\n",
        "Vector search (Pinecone), hybrid search (BM25 + vector), and reranking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def vector_search_pinecone(query: str, top_k: int = 10):\n",
        "    q_emb = embed_model.encode(query, show_progress_bar=False).tolist()\n",
        "    res = index.query(vector=q_emb, top_k=top_k, include_metadata=True)\n",
        "    return res.get(\"matches\", [])\n",
        "\n",
        "def hybrid_search(query: str, alpha: float = 0.5, top_k: int = 10):\n",
        "    query_tokens = query.split()\n",
        "    if not query_tokens:\n",
        "        return []\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "    if np.max(bm25_scores) - np.min(bm25_scores) > 1e-9:\n",
        "        bm25_norm = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
        "    else:\n",
        "        bm25_norm = np.ones_like(bm25_scores) * 0.5\n",
        "\n",
        "    vector_results = vector_search_pinecone(query, top_k=top_k * 2)\n",
        "    vector_scores = np.zeros(len(semantic_chunks))\n",
        "    for m in vector_results:\n",
        "        idx = int(m[\"id\"])\n",
        "        if 0 <= idx < len(semantic_chunks):\n",
        "            vector_scores[idx] = m.get(\"score\", 0.0)\n",
        "\n",
        "    if np.max(vector_scores) - np.min(vector_scores) > 1e-9:\n",
        "        vector_norm = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores))\n",
        "    else:\n",
        "        vector_norm = np.ones_like(vector_scores) * 0.5\n",
        "\n",
        "    hybrid = alpha * bm25_norm + (1 - alpha) * vector_norm\n",
        "    best_idx = np.argsort(hybrid)[::-1][:top_k]\n",
        "    return [(i, float(hybrid[i]), semantic_chunks[i]) for i in best_idx]\n",
        "\n",
        "def rerank(query: str, candidates, top_k: int = 5):\n",
        "    if not candidates:\n",
        "        return []\n",
        "    pairs = [[query, c[2]] for c in candidates]\n",
        "    inputs = reranker_tokenizer(pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        scores = reranker_model(**inputs).logits.squeeze()\n",
        "    if scores.dim() == 0:\n",
        "        scores = scores.unsqueeze(0)\n",
        "    scored = list(zip(scores.tolist(), candidates))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return scored[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. High-level Retrieval and LLM\n",
        "Define a retrieve() function and initialize the Groq LLM, then provide rag_answer()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.1, max_tokens=1000)\n",
        "print(\"Groq LLM ready\")\n",
        "\n",
        "def retrieve(query: str, top_k: int = 5, alpha: float = 0.5):\n",
        "    hybrid_results = hybrid_search(query, alpha=alpha, top_k=20)\n",
        "    if not hybrid_results:\n",
        "        return []\n",
        "    reranked_results = rerank(query, hybrid_results, top_k=top_k)\n",
        "    return [r[1][2] for r in reranked_results]\n",
        "\n",
        "def rag_answer(query: str, top_k: int = 5) -> str:\n",
        "    ctx = retrieve(query, top_k=top_k)\n",
        "    if not ctx:\n",
        "        return \"Sorry, no relevant information found.\"\n",
        "    context_text = \"\\n\\n\".join([f\"[Document {i+1}]: {t}\" for i, t in enumerate(ctx)])\n",
        "    prompt = f\"\"\"Answer the question based on the following context. If the context does not contain relevant information, please state so.\n\nContext:\n{context_text}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n",
        "    resp = llm.invoke(prompt)\n",
        "    return resp.content if hasattr(resp, 'content') else str(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. LangGraph: Minimal Pipeline (retrieve -> generate)\n",
        "We use LangGraph to orchestrate retrieval followed by generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "    question: str\n",
        "    chat_history: List[str]\n",
        "    top_k: int\n",
        "    documents: List[str]\n",
        "    answer: str\n",
        "\n",
        "def build_graph():\n",
        "    graph = StateGraph(AgentState)\n",
        "\n",
        "    def retrieve_node(state: AgentState) -> AgentState:\n",
        "        question = state.get(\"question\", \"\")\n",
        "        top_k = state.get(\"top_k\", 5)\n",
        "        docs = retrieve(question, top_k=top_k, alpha=0.5)\n",
        "        state[\"documents\"] = docs\n",
        "        return state\n",
        "\n",
        "    def generate_node(state: AgentState) -> AgentState:\n",
        "        question = state.get(\"question\", \"\")\n",
        "        docs = state.get(\"documents\", []) or []\n",
        "        hist = state.get(\"chat_history\", []) or []\n",
        "        context_text = \"\\n\\n\".join([f\"[Document {i+1}]: {t}\" for i, t in enumerate(docs)]) if docs else \"\"\n",
        "        history_text = (\"\\n\\nConversation history (most recent first):\\n\" + \"\\n\".join(hist[-3:])) if hist else \"\"\n",
        "        prompt = f\"\"\"Answer the question using only the information from the provided context and conversation history. If the context does not contain relevant information, say that you do not know.\n\nContext:\n{context_text}{history_text}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n",
        "        resp = llm.invoke(prompt)\n",
        "        state[\"answer\"] = resp.content if hasattr(resp, \"content\") else str(resp)\n",
        "        return state\n",
        "\n",
        "    graph.add_node(\"retrieve\", retrieve_node)\n",
        "    graph.add_node(\"generate\", generate_node)\n",
        "    graph.set_entry_point(\"retrieve\")\n",
        "    graph.add_edge(\"retrieve\", \"generate\")\n",
        "    graph.add_edge(\"generate\", END)\n",
        "    return graph.compile()\n",
        "\n",
        "graph = build_graph()\n",
        "print(\"LangGraph pipeline ready\")\n",
        "\n",
        "def run_graph(question: str, chat_history: Optional[List[str]] = None, top_k: int = 5) -> str:\n",
        "    state = {\n",
        "        \"question\": question,\n",
        "        \"chat_history\": chat_history or [],\n",
        "        \"top_k\": top_k,\n",
        "    }\n",
        "    result = graph.invoke(state)\n",
        "    return result.get(\"answer\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Examples\n",
        "Single-turn and multi-turn examples using both plain RAG and LangGraph pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Single-turn\n",
        "q = \"What is the main contribution of DeepSeek-R1?\"\n",
        "print(\"RAG answer:\")\n",
        "print(rag_answer(q))\n",
        "\n",
        "print(\"\nLangGraph answer:\")\n",
        "print(run_graph(q))\n",
        "\n",
        "# Multi-turn\n",
        "history = []\n",
        "q1 = \"What is DeepSeek-R1?\"\n",
        "a1 = run_graph(q1, history)\n",
        "history.append(f\"User: {q1}\\nAssistant: {a1}\")\n",
        "\n",
        "q2 = \"How does it compare to other models?\"\n",
        "a2 = run_graph(q2, history)\n",
        "history.append(f\"User: {q2}\\nAssistant: {a2}\")\n",
        "\n",
        "print(\"\nMulti-turn results:\")\n",
        "print(\"Turn 1 Answer:\", a1[:300])\n",
        "print(\"Turn 2 Answer:\", a2[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation Utilities\n",
        "Simple LLM-judge metrics and embedding-based recall for quick assessment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq as _Groq\n",
        "import re as _re\n",
        "import numpy as _np\n",
        "\n",
        "class GroqJudge:\n",
        "    \"\"\"Deterministic Groq LLM wrapper for evaluation.\n",
        "    \n",
        "    Use a lower temperature for stable scoring.\"\"\"\n",
        "    def __init__(self, model: str = \"llama-3.3-70b-versatile\"):\n",
        "        self.llm = _Groq(model=model, temperature=0.0)\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        resp = self.llm.invoke(prompt)\n",
        "        return resp.content if hasattr(resp, 'content') else str(resp)\n",
        "\n",
        "judge_llm = GroqJudge()\n",
        "print(\"Evaluation LLM ready\")\n",
        "\n",
        "def _clamp01(x: float) -> float:\n",
        "    return max(0.0, min(1.0, x))\n",
        "\n",
        "def evaluate_faithfulness(answer: str, contexts: list[str]) -> float:\n",
        "    contexts_text = \"\\n\\n\".join([f\"[Document {i+1}]: {c}\" for i, c in enumerate(contexts)])\n",
        "    prompt = f\"\"\"You are an evaluator. Evaluate how faithful the answer is to the context (whether the answer is based on the context without fabrication).\n\nContext:\n{contexts_text}\n\nAnswer:\n{answer}\n\nOutput only a number between 0 and 1.\"\"\"\n",
        "    score_text = judge_llm(prompt)\n",
        "    m = _re.search(r'0?\\.\\d+|1\\.0|0', score_text)\n",
        "    return _clamp01(float(m.group())) if m else 0.0\n",
        "\n",
        "def evaluate_relevance(question: str, answer: str) -> float:\n",
        "    prompt = f\"\"\"Evaluate how well the answer responds to the question.\n\nQuestion:\n{question}\n\nAnswer:\n{answer}\n\nOutput only a number between 0 and 1.\"\"\"\n",
        "    score_text = judge_llm(prompt)\n",
        "    m = _re.search(r'0?\\.\\d+|1\\.0|0', score_text)\n",
        "    return _clamp01(float(m.group())) if m else 0.0\n",
        "\n",
        "def _cosine(a: _np.ndarray, b: _np.ndarray) -> float:\n",
        "    na, nb = _np.linalg.norm(a), _np.linalg.norm(b)\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    return float(_np.dot(a, b) / (na * nb))\n",
        "\n",
        "def evaluate_context_recall(question: str, contexts: list[str]) -> float:\n",
        "    if not contexts:\n",
        "        return 0.0\n",
        "    q_emb = embed_model.encode(question, show_progress_bar=False)\n",
        "    ctx_embs = embed_model.encode(contexts, show_progress_bar=False)\n",
        "    sims = [_cosine(q_emb, c) for c in ctx_embs]\n",
        "    return float(max(sims)) if sims else 0.0\n",
        "\n",
        "def evaluate_rag(question: str, answer: str, contexts: list[str]) -> dict:\n",
        "    print(\"Starting RAG evaluation...\")\n",
        "    results = {\n",
        "        \"faithfulness\": evaluate_faithfulness(answer, contexts),\n",
        "        \"relevance\": evaluate_relevance(question, answer),\n",
        "        \"context_recall\": evaluate_context_recall(question, contexts),\n",
        "    }\n",
        "    print(\"Done\")\n",
        "    return results\n",
        "\n",
        "# Example evaluation\n",
        "_q = \"What is DeepSeek-R1?\"\n",
        "_ans = rag_answer(_q)\n",
        "_ctx = retrieve(_q)\n",
        "eval_results = evaluate_rag(_q, _ans, _ctx)\n",
        "eval_results"
      ]
    }
  ]
}
